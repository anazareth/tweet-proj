{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "trudeau = pd.read_csv('data//trudeau_clean.csv', encoding='utf-8')\n",
    "trump = pd.read_csv('data//trump_clean.csv', encoding='utf-8')\n",
    "user_dfs = {'trudeau':trudeau, 'trump':trump}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all links\n",
    "user_dfs['trump']['Tweets'] = user_dfs['trump']['Tweets'].str.replace('https:\\/\\/t\\.co\\/[-a-zA-Z0-9]{1,256}', '')\n",
    "# trim off whitespace from front and back of tweets\n",
    "user_dfs['trump']['Tweets'] = user_dfs['trump']['Tweets'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# exploratory - regex for finding key words/phrases\n",
    "import re\n",
    "\n",
    "target_phrase = ' '\n",
    "\n",
    "matching_tweets=[]\n",
    "for t in trump['Tweets']:\n",
    "    if re.search(target_phrase, t.lower()) is not None:\n",
    "        matching_tweets.append(t)\n",
    "        print(t)\n",
    "        print('-----')\n",
    "print(len(matching_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      kw_jan  freq_jan     kw_feb  freq_feb     kw_mar  freq_mar      kw_apr  \\\n",
      "0      we’re        10      we’re        16      we’re        48       we’re   \n",
      "1        get        10        get        12       help        26        keep   \n",
      "2       keep         8    workers        10    support        21        help   \n",
      "3       food         7       help        10       need        17       here:   \n",
      "4     people         7       keep         9        get        16        need   \n",
      "..       ...       ...        ...       ...        ...       ...         ...   \n",
      "95   benefit         2    subsidy         2  essential         3  principles   \n",
      "96  everyone         2  canadians         2      loved         3        task   \n",
      "97    entire         2     below.         2      first         3       help,   \n",
      "98     check         1     united         2    scotia.         3          go   \n",
      "99  premiers         1      time.         2     better         3       loved   \n",
      "\n",
      "    freq_apr  \n",
      "0         27  \n",
      "1         16  \n",
      "2         15  \n",
      "3         12  \n",
      "4         12  \n",
      "..       ...  \n",
      "95         3  \n",
      "96         3  \n",
      "97         3  \n",
      "98         3  \n",
      "99         3  \n",
      "\n",
      "[100 rows x 8 columns]\n",
      "       kw_jan  freq_jan     kw_feb  freq_feb     kw_mar  freq_mar  \\\n",
      "0       great        71      great        85      great        91   \n",
      "1       never        39  thank_you        33  thank_you        58   \n",
      "2   democrats        32        new        31     people        39   \n",
      "3   president        28  president        30        new        30   \n",
      "4    @foxnews        28       mini        26    working        27   \n",
      "..        ...       ...        ...       ...        ...       ...   \n",
      "95       ever         8        got         8      total         9   \n",
      "96       case         8     always         8       best         9   \n",
      "97     didn’t         8       hope         8        far         9   \n",
      "98      witch         8       must         8   decision         9   \n",
      "99     others         8       fake         8       long         8   \n",
      "\n",
      "         kw_apr  freq_apr  \n",
      "0         great        78  \n",
      "1        people        33  \n",
      "2   white_house        27  \n",
      "3     fake_news        26  \n",
      "4     thank_you        25  \n",
      "..          ...       ...  \n",
      "95      doesn’t         8  \n",
      "96      working         8  \n",
      "97        #maga         7  \n",
      "98         know         7  \n",
      "99        first         7  \n",
      "\n",
      "[100 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))  # common uninteresting words\n",
    "words_to_remove = {'https','amp'}.union(english_stopwords)  # other words I noticed\n",
    "# want to count the following common Trump phrases as one word:\n",
    "trumpisms = {'fake news':'fake_news', 'do nothing democrats':'do_nothing_democrats',\n",
    "            'do nothing dems':'do_nothing_democrats','impeachment hoax':'impeachment_hoax',\n",
    "            'white house':'white_house','thank you':'thank_you','sleepy joe':'sleepy_joe',\n",
    "            'crazy bernie':'crazy_bernie', 'new york':'new_york'}\n",
    "    \n",
    "def tokenize_tweet(tweet):\n",
    "    tweet = tweet.lower()  # only want lowercase letters\n",
    "    for orig, new in trumpisms.items():\n",
    "        tweet = tweet.replace(orig, new)\n",
    "    \n",
    "    words = tweet.split(sep=' ')\n",
    "    keywords = []\n",
    "    bad_chars = [',','.',';','!',':','’','-','&','“','”','...','(',')','?','%', '', '&amp;','به','را','و','ایران']\n",
    "    for w in words:\n",
    "        if w not in words_to_remove and w not in bad_chars:\n",
    "            keywords.append(w)\n",
    "    return keywords\n",
    "\n",
    "users = ['trudeau','trump']\n",
    "kw_dict = {u: None for u in users}\n",
    "for u in users:\n",
    "    keyword_freq = pd.DataFrame()\n",
    "    for m in ['January','February','March','April']: # want cols of DF like this: jt_Jan_word, jt_Jan_freq, jt_Feb_word, jt_Feb_freq, etc\n",
    "        mth = m.lower()[0:3]  # first three letters of lowercase month\n",
    "        mth_tweets = user_dfs[u].loc[user_dfs[u]['Month']==m]['Tweets']\n",
    "        words = pd.Series(np.concatenate([tokenize_tweet(t) for t in mth_tweets])).value_counts()[0:100]\n",
    "        keyword_freq['kw_'+mth] = words.index\n",
    "        keyword_freq['freq_'+mth] = words.values\n",
    "    print(keyword_freq[0:100])\n",
    "    kw_dict[u] = keyword_freq\n",
    "    keyword_freq.to_csv('data//'+u+'_words.csv', header=True, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great',\n",
       " 'people',\n",
       " 'white_house',\n",
       " 'fake_news',\n",
       " 'thank_you',\n",
       " 'states',\n",
       " 'even',\n",
       " 'get',\n",
       " 'conference',\n",
       " 'news',\n",
       " 'thank_you!',\n",
       " 'president',\n",
       " 'new',\n",
       " 'many',\n",
       " 'p.m.',\n",
       " 'back',\n",
       " 'big',\n",
       " 'media',\n",
       " 'good',\n",
       " 'today',\n",
       " 'ventilators',\n",
       " 'country',\n",
       " 'job',\n",
       " 'like',\n",
       " 'total',\n",
       " 'strong',\n",
       " 'never',\n",
       " 'complete',\n",
       " 'far',\n",
       " 'coronavirus',\n",
       " 'small',\n",
       " 'state',\n",
       " 'second',\n",
       " 'hard',\n",
       " 'time',\n",
       " 'congressman',\n",
       " 'got',\n",
       " 'lamestream',\n",
       " 'military',\n",
       " 'way',\n",
       " 'endorsement!',\n",
       " 'american',\n",
       " 'crime',\n",
       " 'businesses',\n",
       " 'would',\n",
       " 'border',\n",
       " 'press',\n",
       " 'said',\n",
       " 'open',\n",
       " 'spoke',\n",
       " 'happy',\n",
       " 'money',\n",
       " 'eastern.',\n",
       " 'york',\n",
       " 'much',\n",
       " 'china',\n",
       " 'want',\n",
       " 'left',\n",
       " 'testing',\n",
       " 'help',\n",
       " 'united',\n",
       " 'done',\n",
       " 'fake_news!',\n",
       " 'u.s.',\n",
       " 'us',\n",
       " 'fighter',\n",
       " 'don’t',\n",
       " 'long',\n",
       " 'do_nothing_democrats',\n",
       " '@nytimes',\n",
       " 'nothing',\n",
       " 'must',\n",
       " 'make',\n",
       " 'failing',\n",
       " 'democrats',\n",
       " 'care',\n",
       " 'invisible',\n",
       " 'work',\n",
       " 'it’s',\n",
       " 'say',\n",
       " 'always',\n",
       " 'wanted',\n",
       " 'governors',\n",
       " 'take',\n",
       " 'million',\n",
       " '@foxnews',\n",
       " 'hospitals',\n",
       " 'countries',\n",
       " 'america',\n",
       " 'radical',\n",
       " 'loves',\n",
       " 'getting',\n",
       " 'night',\n",
       " 'tremendous',\n",
       " 'business',\n",
       " 'doesn’t',\n",
       " 'working',\n",
       " '#maga',\n",
       " 'know',\n",
       " 'first']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(kw_dict[u]['kw_'+mth])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: loop over months and user?\n",
    "\n",
    "mth='apr'\n",
    "u='trudeau'\n",
    "\n",
    "top100_words = list(kw_dict[u]['kw_'+mth])\n",
    "top100_set = set(top100_words)\n",
    "adj_mat = pd.DataFrame(0, index=top100_words, columns=top100_words)  # weighted adjacency matrix (init with 0's)\n",
    "mth_tweets = user_dfs[u].loc[user_dfs[u]['Month']==m]['Tweets']\n",
    "\n",
    "for twt in mth_tweets:  # for each tweet\n",
    "    tweet_words = set(tokenize_tweet(twt)) & top100_set  # set intersection (don't care about words outside top 100)\n",
    "    for wrd in tweet_words:  # for each word in the tweet\n",
    "        wrd_pos = top100_words.index(wrd)\n",
    "        remaining_words = set(tweet_words) - {wrd}\n",
    "        for rem in remaining_words:  # for each other word in the tweet\n",
    "            rem_pos = top100_words.index(rem)\n",
    "            if rem_pos<wrd_pos:  # want consistent ordering so that adj matrix is upper trian\n",
    "                adj_mat.loc[rem,wrd] = adj_mat.loc[rem,wrd] + 1\n",
    "            else:\n",
    "                adj_mat.loc[wrd,rem] = adj_mat.loc[wrd,rem] + 1\n",
    "adj_mat.to_csv('data//'+u+'_adjmat.csv',sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
